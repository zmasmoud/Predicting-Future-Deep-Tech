{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xntqVHNRXSGK",
        "fdrw1Dk3Xhe2",
        "P1uwz_FoHai6",
        "VhO21ZjBXXdB",
        "8RYzC6EUc6ok",
        "hMd6KDvFODQo",
        "G7Cuk6wseqmF",
        "CEisNTepdnyB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "id": "DGqLgzFW6tYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "id": "k2RKx3y96waD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dotenv"
      ],
      "metadata": {
        "id": "q19xvE46CqzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/\"Master semester project\""
      ],
      "metadata": {
        "id": "lDyikaRtA4bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "IrbJhZsH09X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFTjOCU6kZ7p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import faiss\n",
        "import pypdf\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import HfApi, hf_hub_download\n",
        "from io import BytesIO\n",
        "from utils import ChunkLoader, RAGEncoder, retrieve_relevant_chunks, get_all_chunks, show_chunks_statistics, get_index_and_chunks, get_formatted_question_gpt_3_5_Turbo_gpt_4o, get_formatted_instruction_gpt_3_5_Turbo_gpt_4o, get_formatted_prompt_gpt_2, generate_answer\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on: \", device)\n",
        "\n",
        "load_dotenv()  # Loads environment variables from a .env file\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")"
      ],
      "metadata": {
        "id": "VRpWRN1Xj3U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CHUNK_SIZE = 256\n",
        "OVERLAP = int(0.2*MAX_CHUNK_SIZE)\n",
        "\n",
        "# # Setting the LLM used for chunk embedding\n",
        "# chunk_encoder_tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "# chunk_encoder_model = AutoModel.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "# chunk_encoder = RAGEncoder(chunk_encoder_tokenizer, chunk_encoder_model, device)\n",
        "\n",
        "# # Setting the LLM used for question embedding\n",
        "# question_encoder_tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "# question_encoder_model = AutoModel.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "# question_encoder = RAGEncoder(question_encoder_tokenizer, question_encoder_model, device)\n",
        "\n",
        "rag_encoder_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "# rag_encoder_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "# rag_encoder_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "rag_encoder = RAGEncoder(rag_encoder_model.tokenizer, rag_encoder_model, device)\n",
        "chunk_encoder = rag_encoder\n",
        "question_encoder = rag_encoder"
      ],
      "metadata": {
        "id": "YA-Mx0-oW2Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG preparation: chunking, embedding and uploading to HF hub"
      ],
      "metadata": {
        "id": "xntqVHNRXSGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# folder path containing subfolders which contain the rag documents (the parent folder is divided into subfolders with different timespans)\n",
        "PARENT_FOLDER = \"./RAG documents\"\n",
        "\n",
        "def rag_chunk_embed_and_upload(parent_folder, rag_encoder, max_chunk_size, overlap, prefix, hugging_face_write_token, show_statistics):\n",
        "  api = HfApi(token=hugging_face_write_token)\n",
        "\n",
        "  for sub_folder in tqdm(os.listdir(parent_folder), desc=\"reading parent folder \" + parent_folder + \" ...\"):\n",
        "    #chunking the documents of the current sub_folder\n",
        "    all_chunks = get_all_chunks(parent_folder + \"/\" + sub_folder, rag_encoder.tokenizer, max_chunk_size, overlap)\n",
        "\n",
        "    if show_statistics:\n",
        "      show_chunks_statistics(all_chunks, rag_encoder.tokenizer)\n",
        "\n",
        "    # transform the chunks list into a jsonl file (we use BytesIO to avoid having to store the file locally)\n",
        "    all_chunks_jsonl_content = \"\\n\".join([json.dumps({\"chunk\": chunk}) for chunk in all_chunks])\n",
        "    all_chunks_jsonl_file = BytesIO(all_chunks_jsonl_content.encode(\"utf-8\"))\n",
        "\n",
        "    # upload the jsonl file\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=all_chunks_jsonl_file,\n",
        "        path_in_repo=prefix + \"_chunks_\" + sub_folder + \".jsonl\",\n",
        "        repo_id=\"ziedM/rag_dataset\",\n",
        "        repo_type=\"dataset\",\n",
        "    )\n",
        "\n",
        "    # vectorize the chunks (again we use BytesIO for the same reason as above)\n",
        "    all_vectors_npy_file = BytesIO()\n",
        "    all_vectors_npy_content = np.vstack([rag_encoder.encode_text(chunk) for chunk in tqdm(all_chunks, desc=\"vectorizing the chunks ...\")])\n",
        "    np.save(all_vectors_npy_file, all_vectors_npy_content)\n",
        "    all_vectors_npy_file.seek(0)  # set the offset back to the beginning of the stream\n",
        "\n",
        "    # upload the embeddings/vectors (we choose numpy as we're using faiss vector DB for similarity search later in the RAG pipeline)\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=all_vectors_npy_file,\n",
        "        path_in_repo=prefix + \"_embeddings_\" + sub_folder + \".npy\",\n",
        "        repo_id=\"ziedM/rag_dataset\",\n",
        "        repo_type=\"dataset\",\n",
        "    )\n",
        "\n",
        "rag_chunk_embed_and_upload(PARENT_FOLDER, chunk_encoder, MAX_CHUNK_SIZE, OVERLAP, \"sentence_transformer\", HUGGING_FACE_TOKEN, True)"
      ],
      "metadata": {
        "id": "U04de2nmWuHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation Setup"
      ],
      "metadata": {
        "id": "fdrw1Dk3Xhe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the LLMs used for generation (we are using 3 LLMs GPT2, GPT-3.5 Turbo and GPT-4o)\n",
        "\n",
        "# GPT2 from HF: (knowledge cutoff: November 2019)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-xl\").to(device)\n",
        "\n",
        "# This OpenAI client is used to chat with GPT-3.5 Turbo (knowledge cutoff: September 2021) and GPT-4o\n",
        "# (knowledge cutoff: October 2023) through OpenAI's platform (as these models are not available publicly)\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "8jt1tz2auH06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instruction 1.1 is used to get the prediction and instruction 1.2 is used to get the assesment\n",
        "TOP_K_FIRST_MODEL = 3\n",
        "TOP_K_SECOND_MODEL = 5\n",
        "TOP_K_THIRD_MODEL = 7\n",
        "RESULT_DIR = \"result_sentence_transformer\" # or result_facebook_dpr\n",
        "\n",
        "role = \"You are an expert in deep technologies.\"\n",
        "# Used for No-RAG\n",
        "instruction_1_0 = \"Answer the question below.\"\n",
        "instruction_1_1 = \"Answer the question below. Keep your response concise (maximum 15 lines).\"\n",
        "# Used for critical thinking\n",
        "# instruction_1_2 = \"\"\"Based on your current knowledge, provide a short analysis of how accurate\n",
        "# the following prediction about deep technologies in {} is, citing\n",
        "# relevant developments if possible. Specifically, assess whether the\n",
        "# technologies mentioned gained significant adoption or traction, were\n",
        "# delayed, overestimated, or abandoned.\n",
        "\n",
        "# Prediction: {}\n",
        "\n",
        "# Assessment: \"\"\"\n",
        "instruction_1_2 = \"\"\"Based on your current knowledge, assess whether the\n",
        "technologies mentioned in the following prediction are ranked correctly:\n",
        "from most to least likely to become emergent in the year {}.\n",
        "\n",
        "Prediction: {}\n",
        "\n",
        "Assessment: \"\"\"\n",
        "# Used for RAG\n",
        "instruction_2_0 = \"Using the provided context, answer the question below.\"\n",
        "instruction_2_1 = \"Using the provided context, answer the question below. Keep your response concise (maximum 15 lines).\"\n",
        "\n",
        "# question_gpt_2 = \"\"\"What are the top 7 deep technologies that are likely to be the\n",
        "# most impactful in 2021? Rank from most to least likely to become impactful.\"\"\"\n",
        "question_gpt_2 = \"\"\"Rank the following 7 deep technologies in descending order of emergence:\n",
        "from most to least likely to become impactful in 2021. The 7 deep technologies are:\n",
        "AI, Quantum Computing, AR & VR, Robotics, Biotech, New Materials, Electronics & Photonics.\"\"\"\n",
        "# question_gpt_3_5_turbo_gpt_4o = \"\"\"What are the top 7 deep technologies that are likely to be the\n",
        "# most impactful in {}? Rank from most to least likely to become impactful.\n",
        "# For each: name the tech and briefly state why it's emerging.\"\"\"\n",
        "question_gpt_3_5_turbo_gpt_4o = \"\"\"Rank the following 7 deep technologies in descending order of emergence:\n",
        "from most to least likely to become impactful in {}. The 7 deep technologies are:\n",
        "AI, Quantum Computing, AR & VR, Robotics, Biotech, New Materials, Electronics & Photonics.\n",
        "For each: briefly justify your chosen ranking.\"\"\"\n",
        "question_gpt_3_5_turbo = question_gpt_3_5_turbo_gpt_4o.format(2023)\n",
        "question_gpt_4o = question_gpt_3_5_turbo_gpt_4o.format(2025)"
      ],
      "metadata": {
        "id": "_ApEYYC3OOv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting GPT-2 RAG contexts and predictions\n",
        "### Getting GPT-2 predictions using the HF model is slow on a CPU. Thus, we make use of colab's T4 GPU for faster generations. The obtained predictions are saved to a json file (gpt-2-predictions.json).\n",
        "### However, to obtain the contexts (when doing RAG), we have to use faiss-cpu as colab doesn't support faiss-gpu. Thus, the contexts were obtained first with the CPU and the predictions were then obtained using the GPU. The contexts are saved to a json file (gpt-2-contexts.json).This is why this step is done separately from the pipelines in ideas 0 to 4."
      ],
      "metadata": {
        "id": "P1uwz_FoHai6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# context for rag without reranking\n",
        "# index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2020.jsonl\", \"sentence_transformer_embeddings_past_2020.npy\")\n",
        "# relevant_chunks = retrieve_relevant_chunks(question_gpt_2, index, chunks, question_encoder, topk=TOP_K_FIRST_MODEL, normalize=True)\n",
        "# context = \"\\n\".join(relevant_chunks)\n",
        "\n",
        "# context for rag with reranking\n",
        "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2020.jsonl\", \"sentence_transformer_embeddings_past_2020.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_2, index, chunks, question_encoder, topk=25, normalize=True)\n",
        "scores = cross_encoder_model.predict([(question_gpt_2, chunk) for chunk in relevant_chunks])\n",
        "# rerank the chunks: sort the chunks in descending order accroding to their scores\n",
        "context = \"\\n\".join(list(np.array(relevant_chunks)[np.flip(np.argsort(scores))])[:TOP_K_FIRST_MODEL])"
      ],
      "metadata": {
        "id": "QC2NbUqyJp67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(context)"
      ],
      "metadata": {
        "id": "nfu9wDj8tJI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_gpt_2_2019_2020 = context"
      ],
      "metadata": {
        "id": "XTZJ_1vooPbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_gpt_2_past_2020 = context"
      ],
      "metadata": {
        "id": "uNMFizZisbZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_gpt_2_2019_2020_reranked = context"
      ],
      "metadata": {
        "id": "sCpsvSKksdS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_gpt_2_past_2020_reranked = context"
      ],
      "metadata": {
        "id": "-hc6HBRYsYGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts = {\n",
        "    \"context_gpt_2_2019_2020\": context_gpt_2_2019_2020,\n",
        "    \"context_gpt_2_past_2020\": context_gpt_2_past_2020,\n",
        "    \"context_gpt_2_2019_2020_reranked\": context_gpt_2_2019_2020_reranked,\n",
        "    \"context_gpt_2_past_2020_reranked\": context_gpt_2_past_2020_reranked\n",
        "}\n",
        "\n",
        "with open(\"gpt-2-contexts-subapproach2.json\", \"w\") as f:\n",
        "  json.dump(contexts, f)"
      ],
      "metadata": {
        "id": "Ud4t_Z_7skar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"gpt-2-contexts-subapproach2.json\", \"r\") as f:\n",
        "    context_data = json.load(f)\n",
        "\n",
        "context = context_data[\"context_gpt_2_past_2020_reranked\"]"
      ],
      "metadata": {
        "id": "ghAaOnRm_CQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if no rag\n",
        "# prompt_1 = get_formatted_prompt_gpt_2(role + \" \" + instruction_1_0, context=None, question=question_gpt_2, RAG=False)\n",
        "# prediction_1 = generate_answer(prompt_1, tokenizer, model, device)\n",
        "# prediction_1 = prediction_1[prediction_1.index(\"Answer: \")+8:].strip(' ')\n",
        "\n",
        "# if rag\n",
        "prompt_1 = get_formatted_prompt_gpt_2(role + \" \" + instruction_2_0, context, question_gpt_2, RAG=True)\n",
        "prediction_1 = generate_answer(prompt_1, tokenizer, model, device)\n",
        "prediction_1 = prediction_1[prediction_1.index(\"Answer: \")+8:].strip(' ')"
      ],
      "metadata": {
        "id": "mK4S3f-o84sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction_1)"
      ],
      "metadata": {
        "id": "dX7GsWAdvPkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_1_gpt_2_no_rag = prediction_1"
      ],
      "metadata": {
        "id": "nWKn-m4Zu3JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_1_gpt_2_rag_2019_2020 = prediction_1"
      ],
      "metadata": {
        "id": "En25cFKru3yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_1_gpt_2_rag_past_2020 = prediction_1"
      ],
      "metadata": {
        "id": "Ha9H7GRJu3-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_1_gpt_2_rag_2019_2020_reranked = prediction_1"
      ],
      "metadata": {
        "id": "U9M9xbCSvDQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_1_gpt_2_rag_past_2020_reranked = prediction_1"
      ],
      "metadata": {
        "id": "lBNvMmTJvDfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = {\n",
        "    \"prediction_1_gpt_2_no_rag\": prediction_1_gpt_2_no_rag,\n",
        "    \"prediction_1_gpt_2_rag_2019_2020\": prediction_1_gpt_2_rag_2019_2020,\n",
        "    \"prediction_1_gpt_2_rag_past_2020\": prediction_1_gpt_2_rag_past_2020,\n",
        "    \"prediction_1_gpt_2_rag_2019_2020_reranked\": prediction_1_gpt_2_rag_2019_2020_reranked,\n",
        "    \"prediction_1_gpt_2_rag_past_2020_reranked\": prediction_1_gpt_2_rag_past_2020_reranked\n",
        "}\n",
        "\n",
        "with open(\"gpt-2-predictions-subapproach2.json\", \"w\") as f:\n",
        "  json.dump(predictions, f)"
      ],
      "metadata": {
        "id": "0x_AE5KMvOvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idea 0"
      ],
      "metadata": {
        "id": "VhO21ZjBXXdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt 1 is for gpt-2 to get prediction 1.\n",
        "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
        "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
        "# prompt 4 is for gpt-4o to get assesment 2.\n",
        "# prompt 5 is for gpt-4o to get prediction 3\n",
        "\n",
        "with open(\"gpt-2-predictions-subapproach1.json\", \"r\") as f:\n",
        "    gpt_2_predictions = json.load(f)\n",
        "\n",
        "prediction_1 = gpt_2_predictions[\"prediction_1_gpt_2_no_rag\"]\n",
        "\n",
        "\n",
        "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
        "assessment_1 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "prompt_3 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_3_5_turbo)\n",
        "prediction_2 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_1},\n",
        "  {\"role\": \"system\", \"content\": instruction_1_1 + \"\\n\\n\"},\n",
        "  {\"role\": \"user\", \"content\": prompt_3}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
        "assessment_2 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "prompt_5 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_4o)\n",
        "prediction_3 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_2},\n",
        "  {\"role\": \"system\", \"content\": instruction_1_1 + \"\\n\\n\"},\n",
        "  {\"role\": \"user\", \"content\": prompt_5}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "result = {\n",
        "    \"prediction_1\": prediction_1,\n",
        "    \"assessment_1\": assessment_1,\n",
        "    \"prediction_2\": prediction_2,\n",
        "    \"assessment_2\": assessment_2,\n",
        "    \"prediction_3\": prediction_3\n",
        "}\n",
        "\n",
        "with open(RESULT_DIR+\"/\"+\"results_idea0_subapproach1.json\", \"w\") as f:\n",
        "  json.dump(result, f)"
      ],
      "metadata": {
        "id": "V6Ha2PNNXbhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idea 1\n"
      ],
      "metadata": {
        "id": "8RYzC6EUc6ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt 1 is for gpt-2 to get prediction 1.\n",
        "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
        "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
        "# prompt 4 is for gpt-4o to get assesment 2.\n",
        "# prompt 5 is for gpt-4o to get prediction 3\n",
        "\n",
        "with open(\"gpt-2-predictions-subapproach1.json\", \"r\") as f:\n",
        "    gpt_2_predictions = json.load(f)\n",
        "\n",
        "prediction_1 = gpt_2_predictions[\"prediction_1_gpt_2_rag_2019_2020\"]\n",
        "\n",
        "\n",
        "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
        "assessment_1 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_2021_2022.jsonl\", \"sentence_transformer_embeddings_2021_2022.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_3_5_turbo, index, chunks, question_encoder, topk=TOP_K_SECOND_MODEL, normalize=True)\n",
        "context = \"\\n\".join(relevant_chunks)\n",
        "prompt_3 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_3_5_turbo)\n",
        "prediction_2 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_1},\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(\"\", instruction_2_1, context, True)},\n",
        "  {\"role\": \"user\", \"content\": prompt_3}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
        "assessment_2 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_2023_2024.jsonl\", \"sentence_transformer_embeddings_2023_2024.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_4o, index, chunks, question_encoder, topk=TOP_K_THIRD_MODEL, normalize=True)\n",
        "context = \"\\n\".join(relevant_chunks)\n",
        "prompt_5 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_4o)\n",
        "prediction_3 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_2},\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(\"\", instruction_2_1, context, True)},\n",
        "  {\"role\": \"user\", \"content\": prompt_5}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "result = {\n",
        "    \"prediction_1\": prediction_1,\n",
        "    \"assessment_1\": assessment_1,\n",
        "    \"prediction_2\": prediction_2,\n",
        "    \"assessment_2\": assessment_2,\n",
        "    \"prediction_3\": prediction_3\n",
        "}\n",
        "\n",
        "with open(RESULT_DIR+\"/\"+\"results_idea1_subapproach1.json\", \"w\") as f:\n",
        "  json.dump(result, f)"
      ],
      "metadata": {
        "id": "t8yP08Ixc-QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idea 2"
      ],
      "metadata": {
        "id": "hMd6KDvFODQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt 1 is for gpt-2 to get prediction 1.\n",
        "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
        "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
        "# prompt 4 is for gpt-4o to get assesment 2.\n",
        "# prompt 5 is for gpt-4o to get prediction 3\n",
        "\n",
        "with open(\"gpt-2-predictions-subapproach1.json\", \"r\") as f:\n",
        "    gpt_2_predictions = json.load(f)\n",
        "\n",
        "prediction_1 = gpt_2_predictions[\"prediction_1_gpt_2_rag_past_2020\"]\n",
        "\n",
        "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
        "assessment_1 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2022.jsonl\", \"sentence_transformer_embeddings_past_2022.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_3_5_turbo, index, chunks, question_encoder, topk=TOP_K_SECOND_MODEL, normalize=True)\n",
        "context = \"\\n\".join(relevant_chunks)\n",
        "prompt_3 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_3_5_turbo)\n",
        "prediction_2 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_1},\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(\"\", instruction_2_1, context, True)},\n",
        "  {\"role\": \"user\", \"content\": prompt_3}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
        "assessment_2 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2024.jsonl\", \"sentence_transformer_embeddings_past_2024.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_4o, index, chunks, question_encoder, topk=TOP_K_THIRD_MODEL, normalize=True)\n",
        "context = \"\\n\".join(relevant_chunks)\n",
        "prompt_5 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_4o)\n",
        "prediction_3 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_2},\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(\"\", instruction_2_1, context, True)},\n",
        "  {\"role\": \"user\", \"content\": prompt_5}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "result = {\n",
        "    \"prediction_1\": prediction_1,\n",
        "    \"assessment_1\": assessment_1,\n",
        "    \"prediction_2\": prediction_2,\n",
        "    \"assessment_2\": assessment_2,\n",
        "    \"prediction_3\": prediction_3\n",
        "}\n",
        "\n",
        "with open(RESULT_DIR+\"/\"+\"results_idea2_subapproach1.json\", \"w\") as f:\n",
        "  json.dump(result, f)"
      ],
      "metadata": {
        "id": "3CrsQYAzOHLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idea 3"
      ],
      "metadata": {
        "id": "G7Cuk6wseqmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt 1 is for gpt-2 to get prediction 1.\n",
        "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
        "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
        "# prompt 4 is for gpt-4o to get assesment 2.\n",
        "# prompt 5 is for gpt-4o to get prediction 3\n",
        "\n",
        "with open(\"gpt-2-predictions-subapproach1.json\", \"r\") as f:\n",
        "    gpt_2_predictions = json.load(f)\n",
        "\n",
        "prediction_1 = gpt_2_predictions[\"prediction_1_gpt_2_rag_2019_2020_reranked\"]\n",
        "\n",
        "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
        "assessment_1 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_2021_2022.jsonl\", \"sentence_transformer_embeddings_2021_2022.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_3_5_turbo, index, chunks, question_encoder, topk=25, normalize=True)\n",
        "scores = cross_encoder_model.predict([(question_gpt_3_5_turbo, chunk) for chunk in relevant_chunks])\n",
        "# rerank the chunks: sort the chunks in descending order accroding to their scores\n",
        "context = \"\\n\".join(list(np.array(relevant_chunks)[np.flip(np.argsort(scores))])[:TOP_K_SECOND_MODEL])\n",
        "prompt_3 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_3_5_turbo)\n",
        "prediction_2 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_1},\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(\"\", instruction_2_1, context, True)},\n",
        "  {\"role\": \"user\", \"content\": prompt_3}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
        "assessment_2 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_2023_2024.jsonl\", \"sentence_transformer_embeddings_2023_2024.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_4o, index, chunks, question_encoder, topk=25, normalize=True)\n",
        "scores = cross_encoder_model.predict([(question_gpt_4o, chunk) for chunk in relevant_chunks])\n",
        "# rerank the chunks: sort the chunks in descending order accroding to their scores\n",
        "context = \"\\n\".join(list(np.array(relevant_chunks)[np.flip(np.argsort(scores))])[:TOP_K_THIRD_MODEL])\n",
        "prompt_5 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_4o)\n",
        "prediction_3 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_2},\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(\"\", instruction_2_1, context, True)},\n",
        "  {\"role\": \"user\", \"content\": prompt_5}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "result = {\n",
        "    \"prediction_1\": prediction_1,\n",
        "    \"assessment_1\": assessment_1,\n",
        "    \"prediction_2\": prediction_2,\n",
        "    \"assessment_2\": assessment_2,\n",
        "    \"prediction_3\": prediction_3\n",
        "}\n",
        "\n",
        "with open(RESULT_DIR+\"/\"+\"results_idea3_subapproach1.json\", \"w\") as f:\n",
        "  json.dump(result, f)"
      ],
      "metadata": {
        "id": "Y93hrmlUetYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idea 4"
      ],
      "metadata": {
        "id": "CEisNTepdnyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt 1 is for gpt-2 to get prediction 1.\n",
        "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
        "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
        "# prompt 4 is for gpt-4o to get assesment 2.\n",
        "# prompt 5 is for gpt-4o to get prediction 3\n",
        "\n",
        "with open(\"gpt-2-predictions-subapproach1.json\", \"r\") as f:\n",
        "    gpt_2_predictions = json.load(f)\n",
        "\n",
        "prediction_1 = gpt_2_predictions[\"prediction_1_gpt_2_rag_past_2020_reranked\"]\n",
        "\n",
        "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
        "assessment_1 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2022.jsonl\", \"sentence_transformer_embeddings_past_2022.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_3_5_turbo, index, chunks, question_encoder, topk=25, normalize=True)\n",
        "scores = cross_encoder_model.predict([(question_gpt_3_5_turbo, chunk) for chunk in relevant_chunks])\n",
        "# rerank the chunks: sort the chunks in descending order accroding to their scores\n",
        "context = \"\\n\".join(list(np.array(relevant_chunks)[np.flip(np.argsort(scores))])[:TOP_K_SECOND_MODEL])\n",
        "prompt_3 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_3_5_turbo)\n",
        "prediction_2 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_2},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_1},\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(\"\", instruction_2_1, context, True)},\n",
        "  {\"role\": \"user\", \"content\": prompt_3}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
        "assessment_2 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n",
        "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2024.jsonl\", \"sentence_transformer_embeddings_past_2024.npy\")\n",
        "relevant_chunks = retrieve_relevant_chunks(question_gpt_4o, index, chunks, question_encoder, topk=25, normalize=True)\n",
        "scores = cross_encoder_model.predict([(question_gpt_4o, chunk) for chunk in relevant_chunks])\n",
        "# rerank the chunks: sort the chunks in descending order accroding to their scores\n",
        "context = \"\\n\".join(list(np.array(relevant_chunks)[np.flip(np.argsort(scores))])[:TOP_K_THIRD_MODEL])\n",
        "prompt_5 = get_formatted_question_gpt_3_5_Turbo_gpt_4o(question_gpt_4o)\n",
        "prediction_3 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction_1_1, None, False)},\n",
        "  {\"role\": \"user\", \"content\": prompt_4},\n",
        "  {\"role\": \"assistant\", \"content\": assessment_2},\n",
        "  {\"role\": \"system\", \"content\": get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(\"\", instruction_2_1, context, True)},\n",
        "  {\"role\": \"user\", \"content\": prompt_5}\n",
        "]\n",
        ").choices[0].message.content\n",
        "\n",
        "\n",
        "result = {\n",
        "    \"prediction_1\": prediction_1,\n",
        "    \"assessment_1\": assessment_1,\n",
        "    \"prediction_2\": prediction_2,\n",
        "    \"assessment_2\": assessment_2,\n",
        "    \"prediction_3\": prediction_3\n",
        "}\n",
        "\n",
        "with open(RESULT_DIR+\"/\"+\"results_idea4_subapproach1.json\", \"w\") as f:\n",
        "  json.dump(result, f)"
      ],
      "metadata": {
        "id": "iM0_rDyGdqWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "fUSu9kG4cpLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(RESULT_DIR+\"/\"+\"results_idea4_subapproach1.json\", \"r\") as f:\n",
        "    predictions = json.load(f)\n",
        "\n",
        "print(f\"Prediction of GPT-2:\\n {predictions['prediction_1']} \\n\")\n",
        "print(f\"Prediction of GPT-3.5 Turbo:\\n {predictions['prediction_2']} \\n\")\n",
        "print(f\"Prediction of GPT-4o:\\n {predictions['prediction_3']}\")"
      ],
      "metadata": {
        "id": "bVLgfmMPbcvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idea_4_gpt_2_predictions_processed_subapproach1 = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Robotics\", \"Biotech\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "idea_4_gpt_3_5_Turbo_predictions_processed_subapproach1 = [\"AI\", \"Robotics\", \"Biotech\", \"Quantum Computing\", \"AR & VR\", \"Electronics & Photonics\", \"New Materials\"]\n",
        "idea_4_gpt_4o_predictions_processed_subapproach1 = [\"AI\", \"Biotech\", \"Robotics\", \"Quantum Computing\", \"AR & VR\", \"Electronics & Photonics\", \"New Materials\"]\n",
        "\n",
        "idea_3_gpt_2_predictions_processed_subapproach1 = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Robotics\", \"Biotech\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "idea_3_gpt_3_5_Turbo_predictions_processed_subapproach1 = [\"AI\", \"Robotics\", \"Quantum Computing\", \"Biotech\", \"AR & VR\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "idea_3_gpt_4o_predictions_processed_subapproach1 = [\"AI\", \"Biotech\", \"Quantum Computing\", \"Robotics\", \"AR & VR\", \"Electronics & Photonics\", \"New Materials\"]\n",
        "\n",
        "idea_2_gpt_2_predictions_processed_subapproach1 = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Robotics\", \"Biotech\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "idea_2_gpt_3_5_Turbo_predictions_processed_subapproach1 = [\"AI\", \"Quantum Computing\", \"Robotics\", \"Biotech\", \"AR & VR\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "idea_2_gpt_4o_predictions_processed_subapproach1 = [\"AI\", \"Robotics\", \"Biotech\", \"AR & VR\", \"Quantum Computing\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "\n",
        "idea_1_gpt_2_predictions_processed_subapproach1 = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Robotics\", \"Biotech\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "idea_1_gpt_3_5_Turbo_predictions_processed_subapproach1 = [\"AI\", \"Biotech\", \"Quantum Computing\", \"Robotics\", \"AR & VR\", \"Electronics & Photonics\", \"New Materials\"]\n",
        "idea_1_gpt_4o_predictions_processed_subapproach1 = [\"AI\", \"Biotech\", \"Robotics\", \"Quantum Computing\", \"AR & VR\", \"Electronics & Photonics\", \"New Materials\"]\n",
        "\n",
        "idea_0_gpt_2_predictions_processed_subapproach1 = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Robotics\", \"Biotech\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "idea_0_gpt_3_5_Turbo_predictions_processed_subapproach1 = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Biotech\", \"Robotics\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "idea_0_gpt_4o_predictions_processed_subapproach1 = [\"AI\", \"Biotech\", \"AR & VR\", \"Quantum Computing\", \"Robotics\", \"New Materials\", \"Electronics & Photonics\"]"
      ],
      "metadata": {
        "id": "A4W7R1oIXQ1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idea_4_gpt_2_predictions_subapproach2 = [\"Artificial intelligence\", \"Deep learning\", \"Augmented reality\", \"Internet of Things\", \"Cloud computing\", \"Virtual reality\", \"Machine learning\"]\n",
        "idea_4_gpt_3_5_Turbo_predictions_subapproach2 = [\"Human Augmentation\", \"Neural Interfaces\", \"Swarm Intelligence\", \"Citizen Data Science\", \"Autonomous Everything\", \"Biochips\", \"Self-Healing Systems\"]\n",
        "idea_4_gpt_4o_predictions_subapproach2 = [\"Artificial Intelligence\", \"Quantum Computing\", \"5G and Advanced Connectivity\", \"Internet of Things\", \"Blockchain\", \"Augmented and Virtual Reality\", \"Biotechnology and Genomics\"]\n",
        "\n",
        "idea_3_gpt_2_predictions_subapproach2 = [\"Deep learning\", \"Augmented reality\", \"Artificial intelligence\", \"Virtual reality\", \"Deep learning\", \"Augmented reality\", \"Artificial intelligence\"]\n",
        "idea_3_gpt_3_5_Turbo_predictions_subapproach2 = [\"Artificial Intelligence\", \"Quantum Computing\", \"Internet of Things\", \"Augmented Reality\", \"Blockchain\", \"5G Technology\", \"Biotechnology\"]\n",
        "idea_3_gpt_4o_prediction_subapproach2 = [\"Autonomous AI\", \"Pervasive Cloud\", \"Generative AI\", \"Human-Centric Security and Privacy\", \"5G Technology\", \"Quantum Computing\", \"IoT\"]\n",
        "\n",
        "idea_2_gpt_2_predictions_subapproach2 = [\"Artificial Intelligence\", \"Deep Learning\", \"Quantum Computing\", \"Virtual reality\", \"Augmented Reality\", \"Internet of Things\", \"Machine Learning\"]\n",
        "idea_2_gpt_3_5_Turbo_predictions_subapproach2 = [\"Extended Reality\", \"Artificial Intelligence\", \"Quantum Computing\", \"Internet of Things\", \"Blockchain\", \"Edge Computing\", \"Robotics and Automation\"]\n",
        "idea_2_gpt_4o_predictions_subapproach2 = [\"Artificial Intelligence\", \"Internet of Things\", \"Quantum Computing\", \"Blockchain\", \"Edge Computing\", \"Extended Reality\", \"Robotics and Automation\"]\n",
        "\n",
        "idea_1_gpt_2_predictions_subapproach2 = [\"3D Vision\", \"Artificial Intelligence\", \"Deep Learning\", \"Big Data\", \"Machine Learning\", \"Augmented Reality\", \"Blockchain\"]\n",
        "idea_1_gpt_3_5_Turbo_predictions_subapproach2 = [\"Quantum Computing\", \"Edge AI\", \"Synthetic Biology\", \"Robotics Process Automation\", \"Explainable AI\", \"Federated Learning\", \"Neuromorphic Computing\"]\n",
        "idea_1_gpt_4o_predictions_subapproach2 = [\"Autonomous AI\", \"Generative AI\", \"Edge AI\", \"Federated Learning\", \"Synthetic Biology\", \"Human-Centric Security and Privacy\", \"Robotic Process Automation\"]\n",
        "\n",
        "idea_0_gpt_2_predictions_subapproach2 = [\"Artificial Intelligence\", \"Deep Learning\", \"Blockchain\", \"Quantum Computing\", \"3D Printing\", \"Virtual Reality\", \"Augmented Reality\"]\n",
        "idea_0_gpt_3_5_Turbo_predictions_subapproach2 = [\"Artificial Intelligence\", \"Quantum Computing\", \"Internet of Things\", \"Edge Computing\", \"Robotics\", \"Augmented Reality\", \"Biotechnology\"]\n",
        "idea_0_gpt_4o_predictions_subapproach2 = [\"Artificial Intelligence\", \"Quantum Computing\", \"Biotechnology\", \"Internet of Things\", \"Robotics\", \"Edge Computing\", \"Augmented Reality\"]"
      ],
      "metadata": {
        "id": "LUZWlPCNdDa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idea_4_gpt_2_predictions_processed_subapproach2 = [\"AI\", \"AR & VR\", \"IoT\", \"Cloud Computing\"]\n",
        "idea_4_gpt_3_5_Turbo_predictions_processed_subapproach2 = [\"Biotech\", \"AI\", \"Robotics\", \"New Materials\"]\n",
        "idea_4_gpt_4o_predictions_processed_subapproach2 = [\"AI\", \"Quantum Computing\", \"5G\", \"IoT\", \"Blockchain\", \"AR & VR\", \"Biotech\"]\n",
        "\n",
        "idea_3_gpt_2_predictions_processed_subapproach2 = [\"AI\", \"AR & VR\"]\n",
        "idea_3_gpt_3_5_Turbo_predictions_processed_subapproach2 = [\"AI\", \"Quantum Computing\", \"IoT\", \"AR & VR\", \"Blockchain\", \"5G\", \"Biotech\"]\n",
        "idea_3_gpt_4o_predictions_processed_subapproach2 = [\"AI\", \"Cloud Computing\", \"Cybersecurity\", \"5G\", \"Quantum Computing\", \"IoT\"]\n",
        "\n",
        "idea_2_gpt_2_predictions_processed_subapproach2 = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"IoT\"]\n",
        "idea_2_gpt_3_5_Turbo_predictions_processed_subapproach2 = [\"AR & VR\", \"AI\", \"Quantum Computing\", \"IoT\", \"Blockchain\", \"Edge Computing\", \"Robotics\"]\n",
        "idea_2_gpt_4o_predictions_processed_subapproach2 = [\"AI\", \"IoT\", \"Quantum Computing\", \"Blockchain\", \"Edge Computing\", \"AR & VR\", \"Robotics\"]\n",
        "\n",
        "idea_1_gpt_2_predictions_processed_subapproach2 = [\"AI\", \"Cloud Computing\", \"AR & VR\", \"Blockchain\"]\n",
        "idea_1_gpt_3_5_Turbo_predictions_processed_subapproach2 = [\"Quantum Computing\", \"AI\", \"Biotech\", \"Robotics\"]\n",
        "idea_1_gpt_4o_predictions_processed_subapproach2 = [\"AI\", \"Biotech\", \"Cybersecurity\", \"Robotics\"]\n",
        "\n",
        "idea_0_gpt_2_predictions_processed_subapproach2 = [\"AI\", \"Blockchain\", \"Quantum Computing\", \"Additive Manufacturing\", \"AR & VR\"]\n",
        "idea_0_gpt_3_5_Turbo_predictions_processed_subapproach2 = [\"AI\", \"Quantum Computing\", \"IoT\", \"Edge Computing\", \"Robotics\", \"AR & VR\", \"Biotech\"]\n",
        "idea_0_gpt_4o_predictions_processed_subapproach2 = [\"AI\", \"Quantum Computing\", \"Biotech\", \"IoT\", \"Robotics\", \"Edge Computing\", \"AR & VR\"]"
      ],
      "metadata": {
        "id": "8LmM7NH2kIDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_technologies = [\n",
        " 'AI',\n",
        " 'Quantum Computing',\n",
        " 'AR & VR',\n",
        " 'Robotics',\n",
        " 'Biotech',\n",
        " 'New Materials',\n",
        " 'Electronics & Photonics',\n",
        " 'Blockchain',\n",
        " 'Cloud Computing',\n",
        " 'Cybersecurity',\n",
        " 'Edge Computing',\n",
        " 'IoT',\n",
        " '5G',\n",
        " 'Additive Manufacturing'\n",
        " ]\n",
        "\n",
        "all_subterms = [[\"ai\", \"artificial intelligence\", \"machine learning\", \"deep learning\", \"genai\", \"natural language processing\", \"intelligent systems\"],\n",
        "         [\"quantum\", \"qubit\", \"qubits\"],\n",
        "         [\"ar\", \"vr\", \"xr\", \"mr\", \"augmented reality\", \"virtual reality\", \"extended reality\", \"mixed reality\", \"metaverse\"],\n",
        "         [\"robotics\", \"robotic\", \"robots\", \"robot\", \"cobots\", \"cobot\", \"rpa\"],\n",
        "         [\"biopharma\", \"biometrics\", \"biotechnology\", \"biotech\", \"bio\", \"biomolecules\", \"biosystems\", \"biomachines\", \"biocomputing\", \"omics\", \"genetics\", \"neuroscience\", \"life science\", \"genetic engineering\", \"genomics\", \"biology\", \"bioinformatics\"],\n",
        "         [\"nanomaterials\", \"nanomaterial\", \"materials\", \"material\", \"metamaterials\", \"metamaterial\", \"alloy\", \"alloys\", \"superconductors\", \"superconductor\", \"conductive polymers\", \"conductive polymer\", \"photonic crystal\", \"photonic crystal\", \"biodegradable plastics\", \"biodegradable plastic\"],\n",
        "         [\"electronics\", \"electronic\", \"photonics\", \"photonic\", \"integrated circuit\", \"transistor\", \"transistors\", \"microprocessor\", \"microprocessors\", \"microcontroller\", \"microcontrollers\", \"spectroscopy\", \"lidar\", \"led\", \"plasmonics\", \"holography\", \"optics\", \"optical\"],\n",
        "         [\"blockchain\", \"distributed ledger\", \"decentralization\", \"cryptography\", \"smart contracts\", \"consensus mechanisms\"],\n",
        "         [\"cloud\", \"infrastructure as a service\", \"iaas\", \"platform as a service\", \"paas\", \"software as a service\", \"saas\", \"serverless computing\"],\n",
        "         [\"cybersecurity\", \"network security\", \"endpoint protection\", \"encryption\", \"firewall\", \"malware detection\", \"identity and access management\", \"iam\", \"zero trust\", \"phishing prevention\", \"data breach\", \"vulnerability management\"],\n",
        "         [\"edge computing\", \"edge devices\", \"decentralized computing\", \"fog computing\", \"edge analytics\", \"micro data centers\"],\n",
        "         [\"iot\", \"sensors\", \"remote monitoring\"],\n",
        "         [\"5g\", \"massive mimo\", \"beamforming\", \"millimeter wave\", \"mmwave\", \"network slicing\", \"enhanced mobile broadband\", \"embb\"],\n",
        "         ['additive manufacturing', \"3d printing\", \"stereolithography\", \"sla\", \"selective laser sintering\", \"sls\", \"fused deposition modeling\", \"fdm\", \"layer-by-layer fabrication\", \"lbl\", \"powder bed fusion\", \"pbf\"]\n",
        "]\n",
        "technology_to_subterm = dict(zip(unique_technologies, all_subterms))"
      ],
      "metadata": {
        "id": "Z4uIy9-nFwmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "from scipy.sparse import diags\n",
        "from utils import read_pdf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def compute_tf_df_scores(validation_folder_path, terms, tf_normalize):\n",
        "  corpus = []\n",
        "  for file_name in tqdm(os.listdir(validation_folder_path), desc=\"reading validation files \" + validation_folder_path + \" ...\"):\n",
        "    file_content = read_pdf(validation_folder_path + \"/\" + file_name).lower()\n",
        "    corpus.append(file_content)\n",
        "\n",
        "  total_number_of_documents = len(corpus)\n",
        "  vectorizer = TfidfVectorizer(ngram_range=(1, 4), stop_words=stopwords.words('english'), norm=None, use_idf=False)\n",
        "  X = vectorizer.fit_transform(corpus)\n",
        "  vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "  if tf_normalize:\n",
        "    inverse_number_of_unigrams_in_docs = [1/len([elem for elem in vocab[X[i].nonzero()[1]] if len(elem.split())==1]) for i in range(total_number_of_documents)]\n",
        "    normalizer = diags(inverse_number_of_unigrams_in_docs)\n",
        "    X = normalizer @ X\n",
        "\n",
        "  number_of_documents_containing_each_term = [sum([1 if any(sub_term in vocab[X[i].nonzero()[1]] for sub_term in sub_terms) else 0 for i in range(total_number_of_documents)]) for sub_terms in terms]\n",
        "  inverse_document_frequencies = [math.log10((total_number_of_documents+1)/(nb_of_docs+1))+1 for nb_of_docs in number_of_documents_containing_each_term]\n",
        "\n",
        "  # compute the average frequency for each term (i.e. avg nb of occurences of a term in a doc: if tf_normalise is set to True the nb of occurences\n",
        "  # will be normalized by the total number of unigrams in the document), defined as the sum of the number of times the term occured in each document\n",
        "  # of the corpus divided by the number of documents in the corpus. The number of times a term occurs is defined as the sum of the number of times\n",
        "  # the sub_terms corresponding to that term occur.\n",
        "  term_frequencies = [np.sum([X.getcol(vectorizer.vocabulary_.get(sub_term)).toarray() for sub_term in sub_terms if sub_term in vocab])/number_of_documents if number_of_documents else 0 for sub_terms, number_of_documents in zip(terms, number_of_documents_containing_each_term)]\n",
        "  tf_idf_scores = [term_frequencies[i]*inverse_document_frequencies[i] for i in range(len(term_frequencies))]\n",
        "\n",
        "  return tf_idf_scores\n",
        "\n",
        "fixed_technologies = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Robotics\", \"Biotech\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "terms = [technology_to_subterm[technology] for technology in fixed_technologies]\n",
        "\n",
        "# technologies = idea_4_gpt_4o_predictions_processed_subapproach1\n",
        "# terms = [technology_to_subterm[technology] for technology in technologies]\n",
        "tf_normalize = True\n",
        "\n",
        "# validation_folder_path_2019 = \"./validation/2019\"\n",
        "validation_folder_path_2021 = \"./validation/2021\"\n",
        "validation_folder_path_2023 = \"./validation/2023\"\n",
        "validation_folder_path_2025 = \"./validation/2025\"\n",
        "\n",
        "# tf_df_scores_2019 = compute_tf_df_scores(validation_folder_path_2019, terms, tf_normalize)\n",
        "tf_df_scores_2021 = compute_tf_df_scores(validation_folder_path_2021, terms, tf_normalize)\n",
        "tf_df_scores_2023 = compute_tf_df_scores(validation_folder_path_2023, terms, tf_normalize)\n",
        "tf_df_scores_2025 = compute_tf_df_scores(validation_folder_path_2025, terms, tf_normalize)\n",
        "\n",
        "# tf_df_scores_2019_no_zeros = np.array([elem if elem else np.mean(tf_df_scores_2019)/10 for elem in tf_df_scores_2019])\n",
        "# tf_df_scores_2021_no_zeros = np.array([elem if elem else np.mean(tf_df_scores_2021)/10 for elem in tf_df_scores_2021])\n",
        "# tf_df_scores_2023_no_zeros = np.array([elem if elem else np.mean(tf_df_scores_2023)/10 for elem in tf_df_scores_2023])\n",
        "# tf_df_scores_2025_no_zeros = np.array([elem if elem else np.mean(tf_df_scores_2025)/10 for elem in tf_df_scores_2025])\n",
        "\n",
        "# l1_normalized_tf_df_scores_2025 = np.array(tf_df_scores_2025)/np.sum(tf_df_scores_2025)\n",
        "\n",
        "# years = [2025] * len(technologies)\n",
        "years = [2021] * len(fixed_technologies) + [2023] * len(fixed_technologies) + [2025] * len(fixed_technologies)\n",
        "tf_idf_scores = tf_df_scores_2021 + tf_df_scores_2023 + tf_df_scores_2025\n",
        "# tf_idf_scores = l1_normalized_tf_df_scores_2025\n",
        "tf_idf_df = pd.DataFrame({\"Year\": years, \"Term\": fixed_technologies*3, \"TF-IDF\": tf_idf_scores})\n",
        "# tf_idf_df = pd.DataFrame({\"Year\": years, \"Term\": technologies, \"TF-IDF\": tf_idf_scores})"
      ],
      "metadata": {
        "id": "zq5SiYMjQRpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "growth_rates_2019_2021 = (tf_df_scores_2021_no_zeros-tf_df_scores_2019_no_zeros)/tf_df_scores_2019_no_zeros\n",
        "growth_rates_2021_2023 = (tf_df_scores_2023_no_zeros-tf_df_scores_2021_no_zeros)/tf_df_scores_2021_no_zeros\n",
        "growth_rates_2023_2025 = (tf_df_scores_2025_no_zeros-tf_df_scores_2023_no_zeros)/tf_df_scores_2023_no_zeros"
      ],
      "metadata": {
        "id": "pGIr42rZfEd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import colormaps\n",
        "\n",
        "cmap = colormaps['tab20']\n",
        "colors = cmap.colors[:len(unique_technologies)]\n",
        "\n",
        "technology_to_color = dict(zip(unique_technologies, colors))\n",
        "technologies_colors = [technology_to_color[technology] for technology in technologies]\n",
        "\n",
        "sns.barplot(tf_idf_df, x=\"Year\", y=\"TF-IDF\", hue=\"Term\", palette=technologies_colors)\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "plt.ylim(0,1)\n",
        "plt.tight_layout()\n",
        "# plt.savefig(\"tf_idf_normalized_tf_xaxis_year.png\")\n",
        "plt.savefig(\"idea_4_gpt_4o_predictions_subapproach1.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kpi-gVE1jQeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_ordering_string = np.array(technologies)[np.flip(np.argsort(tf_idf_scores))]\n",
        "ground_truth_ordering_numerical = np.arange(1, len(technologies)+1)\n",
        "ground_truth_ordering_dict = dict(zip(ground_truth_ordering_string, ground_truth_ordering_numerical))\n",
        "predicted_ordering_numerical = np.array([ground_truth_ordering_dict[tech] for tech in technologies])"
      ],
      "metadata": {
        "id": "gi-LqowQxs3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_ordering_string_growth_rate = np.array(fixed_technologies)[np.flip(np.argsort(growth_rates_2023_2025))]\n",
        "ground_truth_ordering_numerical_growth_rate = np.arange(1, len(fixed_technologies)+1)\n",
        "ground_truth_ordering_dict_growth_rate = dict(zip(ground_truth_ordering_string_growth_rate, ground_truth_ordering_numerical_growth_rate))\n",
        "predicted_ordering_numerical_growth_rate = np.array([ground_truth_ordering_dict_growth_rate[tech] for tech in technologies])"
      ],
      "metadata": {
        "id": "OH16V6kYdrH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "res = stats.kendalltau(ground_truth_ordering_numerical, predicted_ordering_numerical)\n",
        "print(res.statistic, res.pvalue)"
      ],
      "metadata": {
        "id": "fns2KwZQLs7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = stats.kendalltau(ground_truth_ordering_numerical_growth_rate, predicted_ordering_numerical_growth_rate)\n",
        "print(res.statistic, res.pvalue)"
      ],
      "metadata": {
        "id": "nnPYYnwGjO74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.barplot(tf_df_df, x=\"Year\", y=\"TF-IDF\", hue=\"Term\")\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "plt.tight_layout()\n",
        "technologies_bars_heights = [[bar.get_height() for bar in bars] for bars in ax.containers]      # Extract height of each bar\n",
        "technologies_bars_centers = [[bar.get_x() + bar.get_width()/2 for bar in bars] for bars in ax.containers]\n",
        "technologies_bars_colors = [[bar.get_facecolor() for bar in bars] for bars in ax.containers]\n",
        "tf_df_scores_2019 = np.array([elem if elem else 1 for elem in tf_df_scores_2019])\n",
        "tf_df_scores_2021 = np.array([elem if elem else 1 for elem in tf_df_scores_2021])\n",
        "tf_df_scores_2023 = np.array([elem if elem else 1 for elem in tf_df_scores_2023])\n",
        "tf_df_scores_2025 = np.array([elem if elem else 1 for elem in tf_df_scores_2025])\n",
        "\n",
        "growth_rates_2019_2021 = (tf_df_scores_2021-tf_df_scores_2019)/tf_df_scores_2019\n",
        "growth_rates_2021_2023 = (tf_df_scores_2023-tf_df_scores_2021)/tf_df_scores_2021\n",
        "growth_rates_2023_2025 = (tf_df_scores_2025-tf_df_scores_2023)/tf_df_scores_2023\n",
        "# Add text annotations on top of each bar\n",
        "for technology_bars_heights, technology_bars_centers, technology_bars_colors, growth_rate_1, growth_rate_2 in zip(technologies_bars_heights, technologies_bars_centers, technologies_bars_colors, growth_rates_2021_2023, growth_rates_2023_2025):            # Loop through bars and heights together\n",
        "    bar_1_x, bar_1_y = technology_bars_centers[0], technology_bars_heights[0]\n",
        "    bar_2_x, bar_2_y = technology_bars_centers[1], technology_bars_heights[1]\n",
        "    bar_3_x, bar_3_y = technology_bars_centers[2], technology_bars_heights[2]\n",
        "    curve_color = technology_bars_colors[0]\n",
        "    plt.plot([bar_1_x, bar_2_x], [bar_1_y, bar_2_y], color=curve_color, linewidth=0.5)\n",
        "    plt.plot([bar_2_x, bar_3_x], [bar_2_y, bar_3_y], color=curve_color, linewidth=0.5)\n",
        "    ax.text(\n",
        "        (bar_1_x+bar_2_x)/2,        # X position (center of bar)\n",
        "        (bar_1_y+bar_2_y)/2,                                   # Y position (top of bar)\n",
        "        f'{growth_rate_1:.2f}',                        # Text (format as currency)\n",
        "        ha='center',                             # Horizontal alignment\n",
        "        va='bottom'                              # Vertical alignment\n",
        "    )\n",
        "    ax.text(\n",
        "        (bar_2_x+bar_3_x)/2,        # X position (center of bar)\n",
        "        (bar_2_y+bar_3_y)/2,                                   # Y position (top of bar)\n",
        "        f'{growth_rate_2:.2f}',                        # Text (format as currency)\n",
        "        ha='center',                             # Horizontal alignment\n",
        "        va='bottom'                              # Vertical alignment\n",
        "    )\n",
        "# plt.savefig(\"tf_df_raw_tf_xaxis_year_with_growth_rates.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0fwllAMun-Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(12, 8))\n",
        "from matplotlib import colormaps\n",
        "\n",
        "cmap = colormaps['tab20']\n",
        "colors = cmap.colors[:len(unique_technologies)]\n",
        "\n",
        "technology_to_color = dict(zip(unique_technologies, colors))\n",
        "\n",
        "fixed_technologies = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Robotics\", \"Biotech\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "\n",
        "plot_years = [2021, 2023, 2025]\n",
        "for technology, growth_2021, growth_2023, growth_2025 in zip(fixed_technologies, growth_rates_2019_2021, growth_rates_2021_2023, growth_rates_2023_2025):\n",
        "    plt.plot(plot_years, [growth_2021, growth_2023, growth_2025], marker='o', label=technology, color=technology_to_color[technology])\n",
        "\n",
        "plt.xticks(plot_years)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Growth Rate')\n",
        "# plt.title('Growth Rate of Technologies from 2021 to 2025')\n",
        "plt.legend(title='Technology')\n",
        "plt.grid(True)\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"growth_rates_2021_2023_2025.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-k6e2cYjtsLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import colormaps\n",
        "\n",
        "cmap = colormaps['tab20']\n",
        "colors = cmap.colors[:len(unique_technologies)]\n",
        "\n",
        "technology_to_color = dict(zip(unique_technologies, colors))\n",
        "\n",
        "fixed_technologies = [\"AI\", \"Quantum Computing\", \"AR & VR\", \"Robotics\", \"Biotech\", \"New Materials\", \"Electronics & Photonics\"]\n",
        "tech_colors = [technology_to_color[technology] for technology in fixed_technologies]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(tf_idf_df, x=\"Term\", y=\"TF-IDF\", hue=\"Year\", legend=False)\n",
        "new_xticklabels = [\"2021 2023 2025\\n\" + xticklabel.get_text() for xticklabel in ax.get_xticklabels()]\n",
        "ax.set_xticklabels(new_xticklabels, rotation=0, ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plot_years = [2021, 2023, 2025]\n",
        "technologies_bars_heights = [[ax.containers[i][j].get_height() for i in range(len(plot_years))] for j in range(len(fixed_technologies))]\n",
        "technologies_bars_centers = [[ax.containers[i][j].get_x() + ax.containers[i][j].get_width()/2 for i in range(len(plot_years))] for j in range(len(fixed_technologies))]\n",
        "technologies_bars_colors = [[ax.containers[i][j].get_facecolor() for i in range(len(plot_years))] for j in range(len(fixed_technologies))]\n",
        "# tf_df_scores_2019 = np.array([elem if elem else 1 for elem in tf_df_scores_2019])\n",
        "# tf_df_scores_2021 = np.array([elem if elem else 1 for elem in tf_df_scores_2021])\n",
        "# tf_df_scores_2023 = np.array([elem if elem else 1 for elem in tf_df_scores_2023])\n",
        "# tf_df_scores_2025 = np.array([elem if elem else 1 for elem in tf_df_scores_2025])\n",
        "\n",
        "# growth_rates_2019_2021 = (tf_df_scores_2021-tf_df_scores_2019)/tf_df_scores_2019\n",
        "# growth_rates_2021_2023 = (tf_df_scores_2023-tf_df_scores_2021)/tf_df_scores_2021\n",
        "# growth_rates_2023_2025 = (tf_df_scores_2025-tf_df_scores_2023)/tf_df_scores_2023\n",
        "\n",
        "# Add text annotations on top of each bar\n",
        "for i, (technology_bars_heights, technology_bars_centers, color, growth_rate_1, growth_rate_2, growth_rate_3) in enumerate(zip(technologies_bars_heights, technologies_bars_centers, tech_colors, growth_rates_2019_2021, growth_rates_2021_2023, growth_rates_2023_2025)):\n",
        "    bar_1_x, bar_1_y = technology_bars_centers[0], technology_bars_heights[0]\n",
        "    bar_2_x, bar_2_y = technology_bars_centers[1], technology_bars_heights[1]\n",
        "    bar_3_x, bar_3_y = technology_bars_centers[2], technology_bars_heights[2]\n",
        "\n",
        "    # change the bars colors to stay consistent with previous plots i.e. each technology has the same color\n",
        "    ax.containers[0][i].set_facecolor(color)\n",
        "    ax.containers[1][i].set_facecolor(color)\n",
        "    ax.containers[2][i].set_facecolor(color)\n",
        "\n",
        "    plt.plot([bar_1_x, bar_2_x], [bar_1_y, bar_2_y], color=color, linewidth=0.5)\n",
        "    plt.plot([bar_2_x, bar_3_x], [bar_2_y, bar_3_y], color=color, linewidth=0.5)\n",
        "    ax.text(\n",
        "        bar_1_x,\n",
        "        bar_1_y,\n",
        "        f'{growth_rate_1:.2f}',\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=9\n",
        "    )\n",
        "    ax.text(\n",
        "        bar_2_x,\n",
        "        bar_2_y,\n",
        "        f'{growth_rate_2:.2f}',\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=9\n",
        "    )\n",
        "    ax.text(\n",
        "        bar_3_x,        # X position (center of bar)\n",
        "        bar_3_y,                                   # Y position (top of bar)\n",
        "        f'{growth_rate_3:.2f}',                        # Text (format as currency)\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=9                             # Vertical alignment\n",
        "    )\n",
        "plt.savefig(\"tf_idf_normalized_tf_xaxis_term_with_growth_rates.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4zi58zwoaqIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(tf_idf_df, x=\"Year\", y=\"TF-IDF\", hue=\"Term\", palette=tech_colors)\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"tf_idf_normalized_tf_xaxis_year.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ELNo5bXTo3Yq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
import faiss
import json
import pypdf
import os 
import torch
import numpy as np
import matplotlib.pyplot as plt

from huggingface_hub import HfApi, hf_hub_download
from tqdm.notebook import tqdm

class ChunkLoader:
    @staticmethod
    def load_chunks(file_path):
        chunks = []
        with open(file_path, 'r') as f:
            for line in f:
                chunk = json.loads(line)["chunk"]
                chunks.append(chunk)
        return chunks

class RAGEncoder:
    def __init__(self, tokenizer, model, device):
        self.tokenizer = tokenizer
        self.model = model.to(device)
        self.device = device

    def encode_text(self, text):
        # # this is for facebook dpr 
        # inputs = self.tokenizer(text, return_tensors='pt')
        # with torch.no_grad():
        #     text_embedding = self.model(**inputs).pooler_output.numpy() # if using a cpu 
        text_embedding = self.model.encode(text) # returns a numpy array directly
        
        # return the sentence(s) embedding(s) as a numpy array for either storage on faiss or dot product for similarity calculation 
        return text_embedding #.cpu().numpy()   # in order to return a numpy array we need to bring the tensor back to the cpu side (in case we were using a gpu)

def retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=3, normalize=True):
    question_embedding = question_encoder.encode_text(question)
    print(type(question_embedding))
    question_embedding = question_embedding.reshape(1, -1) if question_embedding.ndim == 1 else question_embedding
    # normalize the question embedding so that inner product between question and chunk = cosine similarity
    if normalize: faiss.normalize_L2(question_embedding)
    # for IP metric, similarities is a representative name for the first element of index.search output tuple  
    # for L2 metric, distances is a representative name for the first element of index.search output tuple
    _, indices = index.search(question_embedding, topk)  
    return [chunks[idx] for idx in indices[0]]

def read_pdf(file_path):
  reader = pypdf.PdfReader(file_path)
  text = ""
  for page in reader.pages:
    text += page.extract_text()
  return text

def get_file_chunks(file_path, tokenizer, max_chunk_size, overlap):
  text = read_pdf(file_path)
  chunks = []
  tokens = tokenizer.encode(text, add_special_tokens=False)
  for i in range(0, len(tokens), max_chunk_size - overlap):
    chunk_tokens = tokens[i:i+max_chunk_size]
    chunk_text = tokenizer.decode(chunk_tokens)
    chunks.append(chunk_text)
  return chunks

def get_all_chunks(folder_path, tokenizer, max_chunk_size, overlap):
  all_chunks = []
  for file_name in tqdm(os.listdir(folder_path), desc="getting chunks from folder " + folder_path + " ..."):
    chunks = get_file_chunks(folder_path + "/" + file_name, tokenizer, max_chunk_size, overlap)
    all_chunks += chunks
  return all_chunks

def show_chunks_statistics(chunks, tokenizer):
  print("The total number of chunks is: ", len(chunks))
  nb_words = []
  nb_chars = []
  nb_tokens = []
  for chunk in chunks:
    nb_words_chunk = len(chunk.split(' '))
    nb_chars_chunk = len(chunk)
    nb_tokens_chunk = len(tokenizer.encode(chunk))
    nb_words.append(nb_words_chunk)
    nb_chars.append(nb_chars_chunk)
    nb_tokens.append(nb_tokens_chunk)


  # Disclaimer: this part of the code was generated by chatGPT as it's only used for visualization
  # purposes for me to check if everything is as intended and is not part of the coding logic.
  fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create 3 subplots in one row

  # Plot the number of words
  axes[0].hist(nb_words, bins=100)
  axes[0].set_title('Number of Words per Chunk')
  axes[0].set_xlabel('Number of Words')
  axes[0].set_ylabel('Frequency')

  # Plot the number of characters
  axes[1].hist(nb_chars, bins=100)
  axes[1].set_title('Number of Characters per Chunk')
  axes[1].set_xlabel('Number of Characters')
  axes[1].set_ylabel('Frequency')

  # Plot the number of tokens
  axes[2].hist(nb_tokens, bins=100)
  axes[2].set_title('Number of Tokens per Chunk')
  axes[2].set_xlabel('Number of Tokens')
  axes[2].set_ylabel('Frequency')

  plt.tight_layout() # Adjust spacing between subplots
  plt.show()

def get_index_and_chunks(rag_chunks_filename, rag_embeddings_filename):
  # Load chunks and their corresponding embeddings
  chunks_file = hf_hub_download(repo_id="ziedM/rag_dataset", filename=rag_chunks_filename, repo_type="dataset")
  chunks = ChunkLoader.load_chunks(chunks_file)
  embeddings_file = hf_hub_download(repo_id="ziedM/rag_dataset", filename=rag_embeddings_filename, repo_type="dataset")
  chunks_embeddings = np.load(embeddings_file)
  # L2 normalization of the embedding vectors
  faiss.normalize_L2(chunks_embeddings)

  # Initialize FAISS index
  embedding_dimension = chunks_embeddings.shape[1]
  # Inner-product-based similarity score (in our case, same as cosine as vectors are normalized)
  index = faiss.IndexFlatIP(embedding_dimension)
  index.add(chunks_embeddings)

  return index, chunks

def get_formatted_prompt_gpt_2(instruction, context, question, RAG):
  formatted_query = (
    instruction + "\n\n"
    + "Context: {}\n\n"
    + "Question: {}\n\n"
    + "Answer: "
  ).format(context, question) if RAG else (
    instruction + "\n\n"
    + "Question: {}\n\n"
    + "Answer: "
  ).format(question)
  return formatted_query

def get_formatted_instruction_gpt_3_5_Turbo_gpt_4o(role, instruction, context, RAG):
  formatted_instruction = (
    instruction + "\n\n"
    + "Context: {}\n\n"
  ).format(context) if RAG else role + " " + instruction + "\n\n"
  return formatted_instruction

def get_formatted_question_gpt_3_5_Turbo_gpt_4o(question):
  return (
    "Question: {}\n\n"
    + "Answer: "
  ).format(question)

def generate_answer(prompt, tokenizer, model, device):
  inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=tokenizer.model_max_length).to(device)
  generated_token_ids = model.generate(**inputs, num_beams=4, do_sample=True, top_k=50, max_new_tokens=tokenizer.model_max_length-inputs.input_ids.shape[1])
  generated_text = tokenizer.decode(generated_token_ids[0])
  return generated_text

